{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a934d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(1, x.size(1), self.hidden_size)\n",
    "        \n",
    "        out, hn = self.rnn(x, h0)\n",
    "        \n",
    "        out = self.fc(hn[-1])\n",
    "        out = torch.sigmoid(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55209e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = [\n",
    "            (\n",
    "                torch.tensor([[-1, 0, 1], [1, -1, 0], [0, 1, -1]], dtype=torch.float),\n",
    "                torch.tensor([1], dtype=torch.float)\n",
    "            ),\n",
    "            (\n",
    "                torch.tensor([[1, 0, -1], [-1, 1, 0], [0, -1, 1]], dtype=torch.float),\n",
    "                torch.tensor([0], dtype=torch.float)\n",
    "            ),\n",
    "            # add more samples as needed\n",
    "        ]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c976733e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "dataset = CustomDataset()\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3c1f274f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 3\n",
    "hidden_size = 10\n",
    "output_size = 1\n",
    "\n",
    "rnn = RNN(input_size, hidden_size, output_size)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c5ed86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 1.4043\n",
      "Epoch [2/100], Loss: 1.2458\n",
      "Epoch [3/100], Loss: 1.1163\n",
      "Epoch [4/100], Loss: 0.9936\n",
      "Epoch [5/100], Loss: 0.8766\n",
      "Epoch [6/100], Loss: 0.7651\n",
      "Epoch [7/100], Loss: 0.6616\n",
      "Epoch [8/100], Loss: 0.5630\n",
      "Epoch [9/100], Loss: 0.4773\n",
      "Epoch [10/100], Loss: 0.4002\n",
      "Epoch [11/100], Loss: 0.3358\n",
      "Epoch [12/100], Loss: 0.2822\n",
      "Epoch [13/100], Loss: 0.2381\n",
      "Epoch [14/100], Loss: 0.2031\n",
      "Epoch [15/100], Loss: 0.1737\n",
      "Epoch [16/100], Loss: 0.1496\n",
      "Epoch [17/100], Loss: 0.1293\n",
      "Epoch [18/100], Loss: 0.1134\n",
      "Epoch [19/100], Loss: 0.1002\n",
      "Epoch [20/100], Loss: 0.0898\n",
      "Epoch [21/100], Loss: 0.0801\n",
      "Epoch [22/100], Loss: 0.0729\n",
      "Epoch [23/100], Loss: 0.0660\n",
      "Epoch [24/100], Loss: 0.0605\n",
      "Epoch [25/100], Loss: 0.0557\n",
      "Epoch [26/100], Loss: 0.0517\n",
      "Epoch [27/100], Loss: 0.0481\n",
      "Epoch [28/100], Loss: 0.0452\n",
      "Epoch [29/100], Loss: 0.0422\n",
      "Epoch [30/100], Loss: 0.0399\n",
      "Epoch [31/100], Loss: 0.0375\n",
      "Epoch [32/100], Loss: 0.0356\n",
      "Epoch [33/100], Loss: 0.0338\n",
      "Epoch [34/100], Loss: 0.0322\n",
      "Epoch [35/100], Loss: 0.0307\n",
      "Epoch [36/100], Loss: 0.0295\n",
      "Epoch [37/100], Loss: 0.0281\n",
      "Epoch [38/100], Loss: 0.0271\n",
      "Epoch [39/100], Loss: 0.0259\n",
      "Epoch [40/100], Loss: 0.0250\n",
      "Epoch [41/100], Loss: 0.0241\n",
      "Epoch [42/100], Loss: 0.0233\n",
      "Epoch [43/100], Loss: 0.0224\n",
      "Epoch [44/100], Loss: 0.0216\n",
      "Epoch [45/100], Loss: 0.0210\n",
      "Epoch [46/100], Loss: 0.0203\n",
      "Epoch [47/100], Loss: 0.0197\n",
      "Epoch [48/100], Loss: 0.0191\n",
      "Epoch [49/100], Loss: 0.0185\n",
      "Epoch [50/100], Loss: 0.0180\n",
      "Epoch [51/100], Loss: 0.0175\n",
      "Epoch [52/100], Loss: 0.0170\n",
      "Epoch [53/100], Loss: 0.0166\n",
      "Epoch [54/100], Loss: 0.0161\n",
      "Epoch [55/100], Loss: 0.0157\n",
      "Epoch [56/100], Loss: 0.0153\n",
      "Epoch [57/100], Loss: 0.0149\n",
      "Epoch [58/100], Loss: 0.0145\n",
      "Epoch [59/100], Loss: 0.0142\n",
      "Epoch [60/100], Loss: 0.0138\n",
      "Epoch [61/100], Loss: 0.0135\n",
      "Epoch [62/100], Loss: 0.0132\n",
      "Epoch [63/100], Loss: 0.0129\n",
      "Epoch [64/100], Loss: 0.0126\n",
      "Epoch [65/100], Loss: 0.0123\n",
      "Epoch [66/100], Loss: 0.0121\n",
      "Epoch [67/100], Loss: 0.0118\n",
      "Epoch [68/100], Loss: 0.0116\n",
      "Epoch [69/100], Loss: 0.0113\n",
      "Epoch [70/100], Loss: 0.0111\n",
      "Epoch [71/100], Loss: 0.0109\n",
      "Epoch [72/100], Loss: 0.0107\n",
      "Epoch [73/100], Loss: 0.0105\n",
      "Epoch [74/100], Loss: 0.0102\n",
      "Epoch [75/100], Loss: 0.0101\n",
      "Epoch [76/100], Loss: 0.0099\n",
      "Epoch [77/100], Loss: 0.0097\n",
      "Epoch [78/100], Loss: 0.0095\n",
      "Epoch [79/100], Loss: 0.0093\n",
      "Epoch [80/100], Loss: 0.0092\n",
      "Epoch [81/100], Loss: 0.0090\n",
      "Epoch [82/100], Loss: 0.0089\n",
      "Epoch [83/100], Loss: 0.0087\n",
      "Epoch [84/100], Loss: 0.0085\n",
      "Epoch [85/100], Loss: 0.0084\n",
      "Epoch [86/100], Loss: 0.0083\n",
      "Epoch [87/100], Loss: 0.0081\n",
      "Epoch [88/100], Loss: 0.0080\n",
      "Epoch [89/100], Loss: 0.0079\n",
      "Epoch [90/100], Loss: 0.0077\n",
      "Epoch [91/100], Loss: 0.0076\n",
      "Epoch [92/100], Loss: 0.0075\n",
      "Epoch [93/100], Loss: 0.0074\n",
      "Epoch [94/100], Loss: 0.0073\n",
      "Epoch [95/100], Loss: 0.0072\n",
      "Epoch [96/100], Loss: 0.0070\n",
      "Epoch [97/100], Loss: 0.0070\n",
      "Epoch [98/100], Loss: 0.0068\n",
      "Epoch [99/100], Loss: 0.0068\n",
      "Epoch [100/100], Loss: 0.0066\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for inputs, targets in dataloader:\n",
    "        rnn.zero_grad()\n",
    "        \n",
    "        outputs = rnn(inputs.permute(1, 0, 2))\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, total_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74652b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "p = 0.5\n",
    "measurement_record_0 = np.load(\"measurement_record_p_{}_Q_{}.npy\".format(p,0))\n",
    "measurement_record_1 = np.load(\"measurement_record_p_{}_Q_{}.npy\".format(p,1))\n",
    "measurement_records = np.concatenate([measurement_record_0,measurement_record_1],axis=0)\n",
    "depth = len(measurement_record_0[0,:,0]) # excluding very last layer containing final measurements\n",
    "L = len(measurement_record_0[0,0,:])\n",
    "num_meas_records_0 = len(measurement_record_0[:,0,0])\n",
    "num_meas_records_1 = len(measurement_record_1[:,0,0])\n",
    "num_meas_records = num_meas_records_0+num_meas_records_1\n",
    "charge_output_0 = np.zeros(num_meas_records_0)\n",
    "charge_output_1 = np.ones(num_meas_records_1)\n",
    "charge_output = np.concatenate([charge_output_0,charge_output_1],axis=0)\n",
    "p = np.random.permutation(num_meas_records) \n",
    "X = measurement_records[p,:,:]\n",
    "y = charge_output[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5031b719",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of SymInts, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/javier/Dropbox/Projects/measurement transitions/learnability_transitions/RNN_decoder.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m rnn \u001b[39m=\u001b[39m RNN(np\u001b[39m.\u001b[39;49mshape(measurement_record_0[\u001b[39m0\u001b[39;49m,:,:]), depth\u001b[39m*\u001b[39;49mL, \u001b[39m2\u001b[39;49m)\n",
      "\u001b[1;32m/Users/javier/Dropbox/Projects/measurement transitions/learnability_transitions/RNN_decoder.ipynb Cell 3\u001b[0m in \u001b[0;36mRNN.__init__\u001b[0;34m(self, input_size, hidden_size, output_size)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39msuper\u001b[39m(RNN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhidden_size \u001b[39m=\u001b[39m hidden_size\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrnn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mRNN(input_size, hidden_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size, output_size)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/torch/nn/modules/rnn.py:425\u001b[0m, in \u001b[0;36mRNN.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    424\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mUnknown nonlinearity \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnonlinearity))\n\u001b[0;32m--> 425\u001b[0m \u001b[39msuper\u001b[39;49m(RNN, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(mode, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/torch/nn/modules/rnn.py:94\u001b[0m, in \u001b[0;36mRNNBase.__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size, device, dtype)\u001b[0m\n\u001b[1;32m     91\u001b[0m real_hidden_size \u001b[39m=\u001b[39m proj_size \u001b[39mif\u001b[39;00m proj_size \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m hidden_size\n\u001b[1;32m     92\u001b[0m layer_input_size \u001b[39m=\u001b[39m input_size \u001b[39mif\u001b[39;00m layer \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m real_hidden_size \u001b[39m*\u001b[39m num_directions\n\u001b[0;32m---> 94\u001b[0m w_ih \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((gate_size, layer_input_size), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     95\u001b[0m w_hh \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((gate_size, real_hidden_size), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[1;32m     96\u001b[0m b_ih \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(gate_size, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of SymInts, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "rnn = RNN(np.shape(measurement_record_0[0,:,:]), depth*L, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6df8a359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:43:48.796338: W tensorflow/c/c_api.cc:291] Operation '{name:'total_5/Assign' id:1091 op device:{requested: '', assigned: ''} def:{{{node total_5/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](total_5, total_5/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/javier/Dropbox/Projects/measurement transitions/learnability_transitions/RNN_decoder.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X16sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, size\u001b[39m=\u001b[39m[\u001b[39m100\u001b[39m, \u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X16sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Train the model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X16sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m model\u001b[39m.\u001b[39;49mfit(data, labels, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X16sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m \u001b[39m# Test the model on some new data\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/javier/Dropbox/Projects/measurement%20transitions/learnability_transitions/RNN_decoder.ipynb#X16sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m test_data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, size\u001b[39m=\u001b[39m[\u001b[39m1\u001b[39m, \u001b[39m10\u001b[39m, \u001b[39m10\u001b[39m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/keras/engine/training_v1.py:854\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_call_args(\u001b[39m\"\u001b[39m\u001b[39mfit\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    853\u001b[0m func \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_training_loop(x)\n\u001b[0;32m--> 854\u001b[0m \u001b[39mreturn\u001b[39;00m func\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    855\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    856\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[1;32m    857\u001b[0m     y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m    858\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    859\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    860\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    861\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    862\u001b[0m     validation_split\u001b[39m=\u001b[39;49mvalidation_split,\n\u001b[1;32m    863\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    864\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    865\u001b[0m     class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[1;32m    866\u001b[0m     sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[1;32m    867\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    868\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    869\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    870\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    871\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[1;32m    872\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[1;32m    873\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[1;32m    874\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/keras/engine/training_generator_v1.py:882\u001b[0m, in \u001b[0;36mGeneratorLikeTrainingLoop.fit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    876\u001b[0m     \u001b[39mif\u001b[39;00m validation_steps:\n\u001b[1;32m    877\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    878\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_steps` should not be specified if \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m`validation_data` is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    880\u001b[0m         )\n\u001b[0;32m--> 882\u001b[0m \u001b[39mreturn\u001b[39;00m fit_generator(\n\u001b[1;32m    883\u001b[0m     model,\n\u001b[1;32m    884\u001b[0m     (x, y, sample_weights),\n\u001b[1;32m    885\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    886\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m    887\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[1;32m    888\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    889\u001b[0m     callbacks\u001b[39m=\u001b[39;49mcallbacks,\n\u001b[1;32m    890\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_data,\n\u001b[1;32m    891\u001b[0m     validation_steps\u001b[39m=\u001b[39;49mvalidation_steps,\n\u001b[1;32m    892\u001b[0m     validation_freq\u001b[39m=\u001b[39;49mvalidation_freq,\n\u001b[1;32m    893\u001b[0m     workers\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[1;32m    894\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[1;32m    895\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[1;32m    896\u001b[0m     steps_name\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39msteps_per_epoch\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    897\u001b[0m )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/keras/engine/training_generator_v1.py:282\u001b[0m, in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m callbacks\u001b[39m.\u001b[39m_call_batch_hook(mode, \u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m, step, batch_logs)\n\u001b[1;32m    281\u001b[0m is_deferred \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m model\u001b[39m.\u001b[39m_is_compiled\n\u001b[0;32m--> 282\u001b[0m batch_outs \u001b[39m=\u001b[39m batch_function(\u001b[39m*\u001b[39;49mbatch_data)\n\u001b[1;32m    283\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(batch_outs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    284\u001b[0m     batch_outs \u001b[39m=\u001b[39m [batch_outs]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/keras/engine/training_v1.py:1157\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[39m# If `self._distribution_strategy` is True, then we are in a replica\u001b[39;00m\n\u001b[1;32m   1153\u001b[0m \u001b[39m# context at this point because of the check above.  `train_on_batch` is\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[39m# being run for each replica by `self._distribution_strategy` and the\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[39m# same code path as Eager is expected to be taken.\u001b[39;00m\n\u001b[1;32m   1156\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_eagerly \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_distribution_strategy:\n\u001b[0;32m-> 1157\u001b[0m     output_dict \u001b[39m=\u001b[39m training_eager_v1\u001b[39m.\u001b[39;49mtrain_on_batch(\n\u001b[1;32m   1158\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   1159\u001b[0m         x,\n\u001b[1;32m   1160\u001b[0m         y,\n\u001b[1;32m   1161\u001b[0m         sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m   1162\u001b[0m         output_loss_metrics\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_output_loss_metrics,\n\u001b[1;32m   1163\u001b[0m     )\n\u001b[1;32m   1164\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[1;32m   1165\u001b[0m         output_dict[\u001b[39m\"\u001b[39m\u001b[39mtotal_loss\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1166\u001b[0m         \u001b[39m+\u001b[39m output_dict[\u001b[39m\"\u001b[39m\u001b[39moutput_losses\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1167\u001b[0m         \u001b[39m+\u001b[39m output_dict[\u001b[39m\"\u001b[39m\u001b[39mmetrics\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m   1168\u001b[0m     )\n\u001b[1;32m   1169\u001b[0m     outputs \u001b[39m=\u001b[39m [_non_none_constant_value(v) \u001b[39mfor\u001b[39;00m v \u001b[39min\u001b[39;00m outputs]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/keras/engine/training_eager_v1.py:342\u001b[0m, in \u001b[0;36mtrain_on_batch\u001b[0;34m(model, inputs, targets, sample_weights, output_loss_metrics)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calculates the loss and gradient updates for one input batch.\u001b[39;00m\n\u001b[1;32m    325\u001b[0m \n\u001b[1;32m    326\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39m      'metrics': list of tensors for metric specified.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m inputs \u001b[39m=\u001b[39m training_utils_v1\u001b[39m.\u001b[39mcast_to_model_input_dtypes(inputs, model)\n\u001b[0;32m--> 342\u001b[0m outs, total_loss, output_losses, masks \u001b[39m=\u001b[39m _process_single_batch(\n\u001b[1;32m    343\u001b[0m     model,\n\u001b[1;32m    344\u001b[0m     inputs,\n\u001b[1;32m    345\u001b[0m     targets,\n\u001b[1;32m    346\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weights,\n\u001b[1;32m    347\u001b[0m     training\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    348\u001b[0m     output_loss_metrics\u001b[39m=\u001b[39;49moutput_loss_metrics,\n\u001b[1;32m    349\u001b[0m )\n\u001b[1;32m    350\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(outs, \u001b[39mlist\u001b[39m):\n\u001b[1;32m    351\u001b[0m     outs \u001b[39m=\u001b[39m [outs]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/keras/engine/training_eager_v1.py:278\u001b[0m, in \u001b[0;36m_process_single_batch\u001b[0;34m(model, inputs, targets, output_loss_metrics, sample_weights, training)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_process_single_batch\u001b[39m(\n\u001b[1;32m    249\u001b[0m     model,\n\u001b[1;32m    250\u001b[0m     inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m     training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    255\u001b[0m ):\n\u001b[1;32m    256\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Calculate the loss and gradient for one input batch.\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \n\u001b[1;32m    258\u001b[0m \u001b[39m       The model weights are updated if training is set to True.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    276\u001b[0m \u001b[39m        ValueError: If the model has no loss to optimize.\u001b[39;00m\n\u001b[1;32m    277\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mwith\u001b[39;00m backend\u001b[39m.\u001b[39meager_learning_phase_scope(\n\u001b[1;32m    279\u001b[0m         \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m training \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    280\u001b[0m     ), training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(model):\n\u001b[1;32m    281\u001b[0m         \u001b[39mwith\u001b[39;00m GradientTape() \u001b[39mas\u001b[39;00m tape:\n\u001b[1;32m    282\u001b[0m             outs, total_loss, output_losses, masks \u001b[39m=\u001b[39m _model_loss(\n\u001b[1;32m    283\u001b[0m                 model,\n\u001b[1;32m    284\u001b[0m                 inputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 training\u001b[39m=\u001b[39mtraining,\n\u001b[1;32m    289\u001b[0m             )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/contextlib.py:135\u001b[0m, in \u001b[0;36m_GeneratorContextManager.__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc\n\u001b[1;32m    134\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgen)\n\u001b[1;32m    136\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[1;32m    137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mgenerator didn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt yield\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mipt/lib/python3.10/site-packages/keras/backend.py:602\u001b[0m, in \u001b[0;36meager_learning_phase_scope\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[39mglobal\u001b[39;00m _GRAPH_LEARNING_PHASES\n\u001b[1;32m    601\u001b[0m \u001b[39massert\u001b[39;00m value \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m}\n\u001b[0;32m--> 602\u001b[0m \u001b[39massert\u001b[39;00m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mexecuting_eagerly_outside_functions()\n\u001b[1;32m    603\u001b[0m global_learning_phase_was_set \u001b[39m=\u001b[39m global_learning_phase_is_set()\n\u001b[1;32m    604\u001b[0m \u001b[39mif\u001b[39;00m global_learning_phase_was_set:\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "\n",
    "## Disable eager execution\n",
    "#tf.disable_eager_execution()\n",
    "\n",
    "# Define the input shape\n",
    "input_shape = [10, 10]\n",
    "\n",
    "# Define the input placeholder\n",
    "inputs = tf.keras.layers.Input(shape=input_shape, name='inputs')\n",
    "\n",
    "# Define the RNN cell\n",
    "cell = tf.keras.layers.SimpleRNN(units=64)\n",
    "\n",
    "# Define the RNN layer\n",
    "outputs = cell(inputs)\n",
    "\n",
    "# Define the fully connected layer\n",
    "dense = tf.keras.layers.Dense(units=1, activation=tf.nn.sigmoid)(outputs)\n",
    "\n",
    "# Define the output\n",
    "output = tf.squeeze(tf.round(dense))\n",
    "\n",
    "# Define the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "\n",
    "# Generate some dummy data\n",
    "data = np.random.randint(-1, 2, size=[100, 10, 10])\n",
    "labels = np.random.randint(0, 2, size=[100, 1])\n",
    "\n",
    "# Train the model\n",
    "model.fit(data, labels, epochs=10, batch_size=32)\n",
    "\n",
    "# Test the model on some new data\n",
    "test_data = np.random.randint(-1, 2, size=[1, 10, 10])\n",
    "result = model.predict(test_data)\n",
    "print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d039d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mipt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "aad86ddac81a43bb81118cddf5163a8939b57853c2b56092156bc0fb02e20aa3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
